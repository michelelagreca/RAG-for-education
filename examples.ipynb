{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG applied to education"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First of all, mount the Google drive content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8qLJu14BBGU",
        "outputId": "9485c977-77ce-4b59-d995-5587e6f68f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-NbtZiBIiv",
        "outputId": "457b444f-1a93-4bb7-a1ba-bf0ae29df994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/817.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/817.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.2\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.0.57-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain<0.2.0,>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.16)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.1.45)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.0.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (8.2.3)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.41->langchain_experimental) (23.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.0.0)\n",
            "Installing collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.0.57\n",
            "Requirement already satisfied: langchain[docarray] in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.0.34)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.1.45)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (0.1.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[docarray]) (8.2.3)\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[docarray])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.10.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (13.7.1)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray])\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.20.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain[docarray]) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain[docarray]) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[docarray]) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[docarray]) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain[docarray]) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[docarray]) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[docarray]) (0.1.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2319658 sha256=1287da3325b4321fb4e1ec97f8e0aa27d62a2ca3b487773e9ad4f59834064125\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: types-requests, hnswlib, docarray\n",
            "Successfully installed docarray-0.32.1 hnswlib-0.8.0 types-requests-2.31.0.20240406\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain_experimental\n",
        "!pip install \"langchain[docarray]\"\n",
        "!pip install PyPDF2;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1s--mfl-BJ5A"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import tiktoken\n",
        "from langchain.text_splitter import CharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Handle the saving of the API Key of OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb30MdMQBJ7p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Prompt the user for their OpenAI API key\n",
        "api_key = input(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Optionally, check that the environment variable was set correctly\n",
        "print(\"OPENAI_API_KEY has been set!\")\n",
        "llm_model = \"gpt-4-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create the function to read the content of a PDF file containing the knowledge base external to the model where RAG needs to retrieve the relevant information. This function loads the text, dividing it into chunks and embedding them, finally inserting them into a vector store. Another function is implemented to create the conversation chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T3r-yzKTedvj"
      },
      "outputs": [],
      "source": [
        "# import a PDF file and convert it to text\n",
        "import PyPDF2\n",
        "def pdf_to_vectorstore(pdf_path):\n",
        "    with open(pdf_file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            text += reader.pages[page_num].extract_text()\n",
        "    txt_file_path = f'/content/drive/MyDrive/{name}.txt'\n",
        "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
        "      txt_file.write(text)\n",
        "    loader = TextLoader(file_path=txt_file_path, encoding=\"utf-8\")\n",
        "    data = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    split_text = text_splitter.split_documents(data)\n",
        "    print(len(split_text[0].page_content))\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectorstore = FAISS.from_documents(split_text, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "def gen_chain(vectorstore):\n",
        "    llm = ChatOpenAI(temperature=0.35, model_name=\"gpt-4-turbo\")\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    return ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Next, a document related to a university course called \"Advanced Operating Systems (AOS)\" is taken in input, and it is saved as vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YqBSKo6ikxI",
        "outputId": "c5fba99e-dd10-4623-aa9a-83fdeb5da68b"
      },
      "outputs": [],
      "source": [
        "name = \"aos\"\n",
        "pdf_file_path = f\"/content/drive/MyDrive/{name}.pdf\"\n",
        "vectorstore = pdf_to_vectorstore(pdf_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The conversation chain related to the previous knowledge base contained into the vector store is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZOmPg9pBxHP",
        "outputId": "2ae0ecce-0471-4344-816b-f999f0efea34"
      },
      "outputs": [],
      "source": [
        "# Create conversation chain\n",
        "conversation_chain = gen_chain(vectorstore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The goal is to create a mind map related to a specific knowledge base. The first step is to give a prompt that tells the LLM to be consistent with the mind map in output by keeping the same structure for all the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The the LLM is asked to summarise the knowledge base into a mind map as JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T_il9FdyBy6r"
      },
      "outputs": [],
      "source": [
        "query = \"Be always consistent with the JSON representation of data you provide me with.\"\n",
        "result = conversation_chain({\"question\": query})\n",
        "# discard answer ...\n",
        "query = \"Summarize the file into a mindmap, generating it in JSON format. Clearly separate different topics.\"\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The JSON is then adjusted with respect to its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LodZ3wDtkf9M"
      },
      "outputs": [],
      "source": [
        "print(map)\n",
        "map = answer[answer.index('{') : len(answer) - answer[::-1].index('}') ]\n",
        "import json\n",
        "def print_json_tree(data, indent=0):\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            # if(indent!=0):\n",
        "            print('    ' * indent + '└── ' + str(key))\n",
        "            # else:\n",
        "            # print(str(key))\n",
        "            print_json_tree(value, indent + 1)\n",
        "    elif isinstance(data, list):\n",
        "        for item in data:\n",
        "            print_json_tree(item, indent)\n",
        "    else:\n",
        "        # if(indent!=0):\n",
        "        print('    ' * indent + '└── ' + str(data))\n",
        "        # else:\n",
        "            # print('    ' * indent + str(data))\n",
        "\n",
        "data = json.loads(map)\n",
        "\n",
        "print_json_tree(data,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A profilation is performed by taking various information of a person. Passing it to the RAG and LLM it is possible to create a mind map in JSON related to the profilation of the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8nhfOmrObYC"
      },
      "outputs": [],
      "source": [
        "age = 20   ## PROFILE: discuss about it at the beginning of the conversation\n",
        "query = f\"Suggest a method for a {age}-year-old student to effectively learn what the file is about. Return a list, with bulletpoints and comments, in JSON.\" # LASCO\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The RAG is performed by asking the LLM to create a specific learning path for the knowledge base (AOS university course) following some information like the current user enrollment, that in this case is university."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdHPihi_EJSC",
        "outputId": "aa948445-2f09-418e-c3ce-2241497d8a4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"learning_journey\": {\n",
            "    \"duration\": \"2 academic years\",\n",
            "    \"semesters\": [\n",
            "      {\n",
            "        \"semester\": 1,\n",
            "        \"duration\": \"6 months\",\n",
            "        \"courses\": [\n",
            "          {\n",
            "            \"course_title\": \"Introduction to Computer Science\",\n",
            "            \"description\": \"Covers basic concepts of computing, programming fundamentals, and an introduction to algorithms.\",\n",
            "            \"topics\": [\n",
            "              \"Basic programming\",\n",
            "              \"Data structures\",\n",
            "              \"Algorithms\"\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"course_title\": \"Introduction to Operating Systems\",\n",
            "            \"description\": \"Introduces basic concepts of operating systems, including process management, memory management, and file systems.\",\n",
            "            \"topics\": [\n",
            "              \"Process management\",\n",
            "              \"Memory management\",\n",
            "              \"File systems\"\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"semester\": 2,\n",
            "        \"duration\": \"6 months\",\n",
            "        \"courses\": [\n",
            "          {\n",
            "            \"course_title\": \"Advanced Operating Systems\",\n",
            "            \"description\": \"Explores advanced topics in operating systems such as concurrency, synchronization, and virtualization.\",\n",
            "            \"topics\": [\n",
            "              \"Concurrency models\",\n",
            "              \"Synchronization techniques\",\n",
            "              \"Virtualization\"\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"course_title\": \"Systems Programming\",\n",
            "            \"description\": \"Focuses on low-level programming, including interaction with hardware, system calls, and performance optimization.\",\n",
            "            \"topics\": [\n",
            "              \"System calls\",\n",
            "              \"I/O devices and drivers\",\n",
            "              \"Performance optimization\"\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"semester\": 3,\n",
            "        \"duration\": \"6 months\",\n",
            "        \"courses\": [\n",
            "          {\n",
            "            \"course_title\": \"Computer Architecture\",\n",
            "            \"description\": \"Covers the design and function of computer hardware, including processors, memory, and input/output systems.\",\n",
            "            \"topics\": [\n",
            "              \"CPU architecture\",\n",
            "              \"Memory hierarchy\",\n",
            "              \"I/O systems\"\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"course_title\": \"Networks and Communications\",\n",
            "            \"description\": \"Introduces network fundamentals, protocols, and communications between computers and devices.\",\n",
            "            \"topics\": [\n",
            "              \"Network protocols\",\n",
            "              \"Data transmission\",\n",
            "              \"Network security\"\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"semester\": 4,\n",
            "        \"duration\": \"6 months\",\n",
            "        \"courses\": [\n",
            "          {\n",
            "            \"course_title\": \"Software Verification and Security\",\n",
            "            \"description\": \"Focuses on methods to ensure software reliability and security, including testing and formal verification techniques.\",\n",
            "            \"topics\": [\n",
            "              \"Software testing\",\n",
            "              \"Formal verification methods\",\n",
            "              \"Security models\"\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"course_title\": \"Capstone Project\",\n",
            "            \"description\": \"A project course where students apply their knowledge to a large-scale software or system project, potentially focusing on operating systems or system security.\",\n",
            "            \"topics\": [\n",
            "              \"Project planning\",\n",
            "              \"Implementation\",\n",
            "              \"Evaluation\"\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# LEARNING JOURNEY --> POSSIBILE ESTENSIONE FUTURA: integriamo testi che parlano di disturbi dell'apprendimento. Facciamo learning journeys tailor-made per gli studenti con difficoltà.\n",
        "# We did prompt engineering to ensure the generation of answers in JSON format. This is very important to realize the integration.\n",
        "schoolOrJob = \"university\"\n",
        "query = f\"Propose a learning journey for a university student to learn about the topics in the document. Specify timeline information. Assume the student has little or no prior knowledge about the topics in the document. Return the result in a JSON.\"\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The new example is related to giving as prompt a specific element of the knoledge base and asking to make exercise related to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnZIJwu6p6L4",
        "outputId": "c94423c5-75fa-4ef1-8732-d72d04fe00a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Below are some exercises to test your knowledge about concurrency primitives in operating systems. Each exercise includes a question and a set of possible answers. The correct answer is indicated in the JSON format provided after the exercises.\n",
            "\n",
            "### Exercises\n",
            "\n",
            "1. **What is a semaphore used for in an operating system?**\n",
            "   - A) To allocate memory efficiently.\n",
            "   - B) To manage access to shared resources by multiple processes.\n",
            "   - C) To increase the priority of a process.\n",
            "   - D) To send messages between processes.\n",
            "\n",
            "2. **Which of the following is NOT a typical use of mutexes?**\n",
            "   - A) To ensure that only one thread accesses a critical section at a time.\n",
            "   - B) To prevent deadlock between processes.\n",
            "   - C) To count the number of times a resource is used.\n",
            "   - D) To control access to a shared resource like a file or memory segment.\n",
            "\n",
            "3. **What is a deadlock in the context of operating systems?**\n",
            "   - A) When the system is locked down for security reasons.\n",
            "   - B) When two or more processes hold resources and each process is waiting for the other to release another resource.\n",
            "   - C) When a process is terminated unexpectedly.\n",
            "   - D) When a process fails to release its memory upon termination.\n",
            "\n",
            "4. **Which concurrency primitive would be most suitable for signaling between processes that a particular event has occurred?**\n",
            "   - A) Mutex\n",
            "   - B) Semaphore\n",
            "   - C) Condition Variable\n",
            "   - D) Spinlock\n",
            "\n",
            "5. **What is the primary disadvantage of using spinlocks in a single-processor system?**\n",
            "   - A) They use less memory than other synchronization primitives.\n",
            "   - B) They can lead to deadlock if not handled properly.\n",
            "   - C) They waste CPU cycles by continuously polling.\n",
            "   - D) They are difficult to implement.\n",
            "\n",
            "### Answers in JSON Format\n",
            "```json\n",
            "{\n",
            "  \"answers\": [\n",
            "    {\n",
            "      \"question\": 1,\n",
            "      \"correct_option\": \"B\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": 2,\n",
            "      \"correct_option\": \"C\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": 3,\n",
            "      \"correct_option\": \"B\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": 4,\n",
            "      \"correct_option\": \"C\"\n",
            "    },\n",
            "    {\n",
            "      \"question\": 5,\n",
            "      \"correct_option\": \"C\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "These exercises cover basic concepts of concurrency primitives such as semaphores, mutexes, deadlocks, condition variables, and spinlocks. The JSON format provides a structured way to review the correct answers.\n"
          ]
        }
      ],
      "source": [
        "# EXERCISE GENERATION\n",
        "context = \"concurrency primitives in operating systems\"\n",
        "query = f\"Test my knowledge about {context}. Propose some exercise, possibly. Return the result in a JSON.\"\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The new example is based on asking the LLM to find extra resources related to the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkGeODxePGEg",
        "outputId": "7a15d2b2-7325-402f-db02-8b5940f035f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"resources\": [\n",
            "    {\n",
            "      \"title\": \"Operating Systems: Three Easy Pieces\",\n",
            "      \"author\": \"Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau\",\n",
            "      \"description\": \"A comprehensive textbook covering the fundamentals of operating systems, including concurrency and memory management.\",\n",
            "      \"url\": \"http://pages.cs.wisc.edu/~remzi/OSTEP/\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Introduction to Operating Systems\",\n",
            "      \"platform\": \"MIT OpenCourseWare\",\n",
            "      \"description\": \"This course teaches the basic operating system abstractions, mechanisms, and their implementations.\",\n",
            "      \"url\": \"https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-828-operating-system-design-and-implementation-fall-2012/\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Linux Kernel Teaching\",\n",
            "      \"author\": \"Robert Love\",\n",
            "      \"description\": \"This is a collection of lectures and labs Linux kernel topics. The lectures focus on theoretical and Linux kernel exploration.\",\n",
            "      \"url\": \"https://github.com/robertlove/linux-kernel-teaching\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Operating System Concepts\",\n",
            "      \"author\": \"Abraham Silberschatz, Peter B. Galvin, Greg Gagne\",\n",
            "      \"description\": \"A textbook for a first course in operating systems at the undergraduate level.\",\n",
            "      \"url\": \"https://www.os-book.com/OS10/\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Virtualization Essentials\",\n",
            "      \"author\": \"Matthew Portnoy\",\n",
            "      \"description\": \"A book that provides a thorough understanding of virtualization and covers various virtualization technologies.\",\n",
            "      \"url\": \"https://www.amazon.com/Virtualization-Essentials-Matthew-Portnoy/dp/1119267722\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Linux Device Drivers\",\n",
            "      \"author\": \"Jonathan Corbet, Alessandro Rubini, and Greg Kroah-Hartman\",\n",
            "      \"description\": \"This book covers all about writing device drivers for the Linux system.\",\n",
            "      \"url\": \"https://lwn.net/Kernel/LDD3/\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# ADDITIONAL RESOURCES\n",
        "query = f\"Provide extra resources, possibly searching the internet for them, to help a student learn about the topics of the document. Return the result in a JSON.\"\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The LLM is asked to create some specific exercises regarding the knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-wJ3UduFlnj",
        "outputId": "1339e23a-2a71-41f7-ce76-2694a26e6969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Here are some challenging exercises related to advanced topics in operating systems, formatted in JSON:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"exercises\": [\n",
            "    {\n",
            "      \"topic\": \"Concurrency and Synchronization\",\n",
            "      \"description\": \"Implement a multithreaded program using POSIX threads that simulates the dining philosophers problem. Ensure that your solution avoids both deadlock and starvation. Use mutexes and condition variables to synchronize access to resources (forks).\",\n",
            "      \"tasks\": [\n",
            "        \"Create a structure to represent each philosopher and fork.\",\n",
            "        \"Implement functions for philosopher actions: think, pick up forks, eat, and put down forks.\",\n",
            "        \"Ensure proper synchronization to avoid deadlock and starvation.\",\n",
            "        \"Test the program with different numbers of philosophers and varying think/eat times to ensure robustness.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"topic\": \"Memory Management\",\n",
            "      \"description\": \"Write a simple memory allocator in C that mimics the behavior of 'malloc' and 'free'. Use explicit free lists, splitting, and coalescing of free blocks.\",\n",
            "      \"tasks\": [\n",
            "        \"Implement the allocator with functions for initialization, allocation, and deallocation.\",\n",
            "        \"Use a first-fit or best-fit strategy for block selection.\",\n",
            "        \"Implement splitting of free blocks on allocation to minimize wasted space.\",\n",
            "        \"Implement coalescing of adjacent free blocks on deallocation to avoid fragmentation.\",\n",
            "        \"Include debug functions to print the current state of the memory to help with testing and validation.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"topic\": \"File Systems\",\n",
            "      \"description\": \"Design and implement a simple file system in a simulated disk environment. Your file system should support basic operations like create, delete, read, and write files.\",\n",
            "      \"tasks\": [\n",
            "        \"Create a simulated disk environment with a fixed-size array of blocks.\",\n",
            "        \"Define and implement the on-disk and in-memory structures (e.g., superblock, inode table, data blocks).\",\n",
            "        \"Implement functions to format the disk, creating an empty file system.\",\n",
            "        \"Implement the file system operations: create, delete, read, and write.\",\n",
            "        \"Write tests to validate each file system operation, including edge cases like disk full, file not found, etc.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"topic\": \"Virtualization\",\n",
            "      \"description\": \"Develop a simple hypervisor in C that can launch and manage a secondary 'guest' operating system in a protected environment.\",\n",
            "      \"tasks\": [\n",
            "        \"Set up the environment for running a secondary OS.\",\n",
            "        \"Implement basic VM exit handlers for CPU instructions that cannot be virtualized directly.\",\n",
            "        \"Handle memory isolation between the hypervisor and the guest OS.\",\n",
            "        \"Implement I/O pass-through for the guest OS to interact with hardware devices.\",\n",
            "        \"Test the hypervisor with a simple guest OS or a real-time operating system.\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "These exercises are designed to challenge students or developers who are familiar with operating system concepts and are comfortable with programming in C or similar languages. Each exercise focuses on a core area of operating systems, providing practical implementation experience.\n"
          ]
        }
      ],
      "source": [
        "# TAKE ONE STEP OF THE LEARNING JOURNEY AND GENERATE AN EXAMPLE EXERCISE\n",
        "context = \"Advanced Topics and Practical Implementation of Operating Systems\"\n",
        "difficulty = \"hard\"\n",
        "query = f\"Test my knowledge about {context}. Propose some exercise with difficulty {difficulty}. Be very specific in the task description. Return the result in a JSON.\"\n",
        "# è vago, chiedi più approfonditamente se vuoi con un altro prompt --> ESTENSIONE: metti i bottoni per chiedere un certo livello di dettaglio e delucidazioni\n",
        "result = conversation_chain({\"question\": query})\n",
        "answer = result[\"answer\"]\n",
        "exercises = answer\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
